{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":201564994,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n%pip install -U transformers\n%pip install -U datasets\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install -U bitsandbytes\n%pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, torch, wandb\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    TrainerCallback,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\nfrom dataclasses import dataclass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    model_name = \"Qwen/Qwen2.5-0.5B\"\n    dataset_name = \"elvispresniy/synthetic-allenai\"\n    new_model = \"MMP-0.5b-it\"\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"eager\"\ncfg = Config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"huggingface_token\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb_api_key\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='SciMMP-0.5b-it', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    cfg.model_name,\n    device_map=\"auto\",\n    attn_implementation=cfg.attn_implementation,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\ntokenizer.padding_side = 'right'\ntokenizer.padding_token = '<|pad|>'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(cfg.dataset_name, split=\"all\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prefix_string = \"<|im_start|>user\\n\"\n# postfix_string = \" As soon as you finished return [EOS] token.<|im_end|>\\n\"\n# eos_token = \"[EOS]\"\nprefix_string = \"<|im_start|>user\\nAnswer only on the subject. Don't be too much verbose. Provide scientific prooves. As soon as the answer is provided return [EOS] token. \"\npostfix_string = \"<|im_end|>\\n\"\nprefix_answer = \"assistant\\n\"\neos_token = \"[EOS]\"\n\ndef preprocess_dataset(row):\n    question = row['question'].split(prefix_string)[-1].split(postfix_string)[0]\n    answer = row['answer'].split(prefix_answer)[-1].split(eos_token)[0]\n    \n    text_template = [\n        {\"role\": \"user\", \"content\": question},\n        {\"role\": \"assistant\", \"content\": answer}\n    ]\n    \n    text = tokenizer.apply_chat_template(text_template, tokenize=False)\n    \n    for k in list(row.keys()):\n        row.pop(k)\n        \n    return {\"text\" : text}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 2025\n\ndataset_sh = dataset.shuffle(seed=SEED).map(preprocess_dataset)\ndataset_sh = dataset_sh.train_test_split(50/11679, seed=SEED)\ndataset_sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(prompt, max_length=50):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n\n    generated = input_ids.to(model.device)\n\n    for _ in range(max_length):\n        outputs = model(generated)\n        logits = outputs.logits\n\n        next_token_logits = logits[:, -1, :]\n        probabilities = torch.softmax(next_token_logits, dim=-1)\n\n        next_token = torch.multinomial(probabilities, num_samples=1)\n\n        generated = torch.cat((generated, next_token), dim=1)\n\n        if next_token.item() == tokenizer.eos_token_id:\n            break\n\n    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n    return generated_text\n\ndef generate_text_it(prompt, max_length=50):\n    prompt = [\n        { \"role\": \"user\", \"content\": prompt },\n    ]\n    prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    generated = input_ids.to(model.device)\n\n    for _ in range(max_length):\n        outputs = model(generated)\n        logits = outputs.logits\n\n        next_token_logits = logits[:, -1, :]\n        probabilities = torch.softmax(next_token_logits, dim=-1)\n\n        next_token = torch.multinomial(probabilities, num_samples=1)\n\n        generated = torch.cat((generated, next_token), dim=1)\n\n        if next_token.item() == tokenizer.eos_token_id:\n            break\n\n    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n    return generated_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_of_prompts = [\"The results of the experiment demonstrated a significant increase in cell proliferation when exposed to \",\n\"According to the theory of general relativity, space-time is curved by \",\n\"The chemical reaction between sodium chloride and water results in the formation of\",\n\"Quantum mechanics suggests that particles exist in multiple states until they are\",\n\"The primary advantage of using CRISPR technology in gene editing is its ability to\",\n\"In a double-blind clinical trial, the control group was administered a placebo while the experimental group received\",\n\"The neural network was trained using a dataset of over one million images, and its performance was evaluated based on \",\n\"Photosynthesis in plants is driven by light energy, which is absorbed by chlorophyll molecules located in the\",\n\"\"\"The Role of Inflammatory Markers in Cardiovascular Disease\nCardiovascular disease is one of the leading causes of mortality worldwide. Recent studies have highlighted the significance of inflammatory markers such as C-reactive protein (CRP) and interleukin-6 (IL-6) in\"\"\",\n\"\"\"Advances in Nanotechnology for Drug Delivery\nNanotechnology has revolutionized the field of drug delivery by enabling the targeted delivery of therapeutics with minimal side effects. In recent years, several novel nanocarriers such as liposomes, dendrimers, and\"\"\",\n\"\"\"Machine Learning Approaches to Predict Protein-Protein Interactions\nProtein-protein interactions (PPIs) play a crucial role in biological processes. However, experimental identification of PPIs is time-consuming and costly. Machine learning models have been developed to\"\"\",\n\"Recent advances in artificial intelligence have led to the development of deep learning models capable of surpassing traditional machine learning algorithms in various tasks. For example, Convolutional Neural Networks (CNNs) have shown remarkable performance in\",\n\"Several studies have investigated the effect of atmospheric CO2 levels on global climate patterns. The seminal work by Smith et al. (2010) demonstrated a clear correlation between \",\n\"A study measured the blood pressure of 200 patients before and after administering a new antihypertensive drug. On average, blood pressure decreased by 15%. This suggests that the drug...\",\n\"In a recent clinical trial, 60% of participants in the treatment group showed symptom improvement, while only 20% of participants in the control group reported similar results. This indicates that\",\n\"Результаты эксперимента продемонстрировали значительное увеличение пролиферации клеток при воздействии\",\n\"Согласно общей теории относительности, пространство-время искривляется на\",]\n\nset_of_prompts_it = [\"Explain the role of transcription factors in gene expression, and give an example of how they can regulate cellular differentiation.\",\n\"Summarize the process of mitosis, focusing on the key stages and their significance for cellular division.\",\n\"Describe the evolutionary significance of horizontal gene transfer in bacteria and its impact on antibiotic resistance.\",\n\"How does the structure of the phospholipid bilayer contribute to the selective permeability of the cell membrane?\",\n\"Compare and contrast the mechanisms of SN1 and SN2 nucleophilic substitution reactions, including the factors that favor each.\",\n\"Explain the concept of Gibbs free energy and its relevance in predicting the spontaneity of chemical reactions.\",\n\"Describe how infrared spectroscopy can be used to identify functional groups in an organic compound.\",\n\"Write a step-by-step explanation of the process of acid-base titration, including how to determine the equivalence point.\",\n\"Explain how the Heisenberg Uncertainty Principle limits our ability to measure both the position and momentum of a particle simultaneously.\",\n\"Describe the fundamental differences between general relativity and quantum mechanics, and explain why they are difficult to reconcile.\",\n\"In the context of thermodynamics, explain the second law and how it relates to entropy in isolated systems.\",\n\"Provide a detailed explanation of how the photoelectric effect supports the quantum theory of light.\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextGenerationCallback(TrainerCallback):\n    def __init__(self):\n        super().__init__()\n        self.cnt = 0\n        self.cnt_it = 0\n\n    def on_evaluate(self, args, state, control, **kwargs):\n        model.eval()\n#         sample_input = set_of_prompts[self.cnt]\n#         self.cnt = (self.cnt + 1) % len(set_of_prompts)\n#         generated_text = generate_text(sample_input)\n#         print(f\"Generated text at step {state.global_step}: {generated_text}\")\n        \n        sample_input = set_of_prompts_it[self.cnt_it]\n        self.cnt_it = (self.cnt_it + 1) % len(set_of_prompts_it)\n        generated_text = generate_text_it(sample_input)\n        print(f\"Generated instruct text at step {state.global_step}: {generated_text}\")\n        \n        !rm -r /kaggle/working/MMP-0.5b-it/checkpoint-*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=cfg.new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    logging_steps=100,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,\n    group_by_length=True,\n    report_to=\"wandb\",\n    run_name=\"SciMMP-0.5b-it\",\n#     lr_scheduler_type='linear'\n    \n    push_to_hub=True,\n    hub_model_id=\"elvispresniy/SciMMP-0.5b-it\",\n    hub_strategy=\"every_save\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_sh[\"train\"],\n    eval_dataset=dataset_sh[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=4096,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n    callbacks=[TextGenerationCallback()]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}