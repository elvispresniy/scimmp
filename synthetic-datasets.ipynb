{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n%pip install -U trl\n%pip install -U transformers\n%pip install -U datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n%pip install --target=/kaggle/working vllm\n%pip install --target=/kaggle/working \"grpcio>=1.60.0\"\n%rm -rf /kaggle/working/ray*\n%pip wheel \"ray>=2.11\" -w /kaggle/working/packages/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"elvispresniy/SciMMP-1.5-it-v2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = 'right'\ntokenizer.padding_token = '<|pad|>'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\n\nllm = LLM(model=\"elvispresniy/SciMMP-1.5-it-v2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndatasets_names = {\n    \"no_robots\": \"HuggingFaceH4/no_robots\",\n    \"allenai\": \"allenai/sciq\"\n}\n\ndatasets_dict = {\n    \"no_robots\": load_dataset(datasets_names[\"no_robots\"]),\n    \"allenai\": load_dataset(datasets_names[\"allenai\"])\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system_prompt_1 = \"Answer only on the subject. Don't be too much verbose. Provide scientific evidence. As soon as the answer is provided return [EOS] token. \"\nsystem_prompt_0 = \" As soon as you finished return [EOS] token.\"\n\ndef preprocess_robots(row):\n    question = row['messages'][0][\"content\"]\n    row_json = [\n        {\"role\": \"user\", \"content\": question + system_prompt_0},\n    ]\n    text = tokenizer.apply_chat_template(row_json, tokenize=False)\n    \n    for k in list(row.keys()):\n        row.pop(k)\n        \n    return {\"text\": text, \"question\": question}\n\ndef preprocess_allenai(row):\n    question =  row[\"question\"]\n    row_json = [\n        {\"role\": \"user\", \"content\": system_prompt_1 + question},\n    ]\n    text = tokenizer.apply_chat_template(row_json, tokenize=False)\n    \n    \n        \n    return {\"text\": text, \"question\": question}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\nSEED = 2025\n\ndatasets_train = [\n    datasets_dict[\"allenai\"][\"train\"].shuffle(SEED).map(preprocess_allenai),\n    datasets_dict[\"no_robots\"][\"train\"].shuffle(SEED).select(range(9_000)).map(preprocess_robots),\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\n\nqa_pairs = []\n\nBATCH_SIZE = 256\nCHECKPOINT_INTERVAL = 1_000_000_000\n\nsampling_params = SamplingParams(top_k=1, max_tokens=512, repetition_penalty=1.1)\n\nfor i in range(0, len(datasets_train[1]), BATCH_SIZE):\n    \n    prompts = datasets_train[1][i:i+BATCH_SIZE]['text']\n    answers = llm.generate(prompts, sampling_params)\n    print(i)\n    \n    for j in range(len(datasets_train[1][i:i+BATCH_SIZE]['text'])):\n        qa_pairs.append(\n            {\"question\": datasets_train[1][i + j]['text'],\n             \"answer\": answers[j].outputs[0].text}\n        )\n        \n    if i % (BATCH_SIZE * CHECKPOINT_INTERVAL) == 0:\n        checkpoint_df = pd.DataFrame(qa_pairs)\n        checkpoint_df.to_csv(f\"checkpoint_{i}.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\nds = Dataset.from_list(qa_pairs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"huggingface_token\")\n\nlogin(token = hf_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.push_to_hub(\"elvispresniy/synthetic-no_robots\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Can you come up with five popular colors for each season? I want to start decorating my bedroom and living room based on the change from winter, summertime, springtime, and autumn each year.\"\n\nprompt = [\n    { \"role\": \"user\", \"content\": prompt },\n]\n\nprompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}